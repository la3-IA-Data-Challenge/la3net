{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('data_challenge': conda)",
   "metadata": {
    "interpreter": {
     "hash": "a76cc6609b19670ed5a5beefd81c358fe89c0b57f12198349bbcd36312527f13"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "\n",
    "from utils.extractors import Encoder\n",
    "from utils.evaluator import Evaluator\n",
    "\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Nombres d'images: 240\n"
     ]
    }
   ],
   "source": [
    "IMAGESDIR = \"./data/near_duplicates/\"\n",
    "print(\"Nombres d'images: {0}\".format(len(os.listdir(IMAGESDIR))))\n",
    "nb_images = len(os.listdir(IMAGESDIR))"
   ]
  },
  {
   "source": [
    "## Load computed clusters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = pickle.load(open(\"./clusters/centroids.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pickle.load(open(\"./clusters/labels.pkl\", \"rb\"))"
   ]
  },
  {
   "source": [
    "## Find closest cluster\n",
    "\n",
    "#### Get closest cluster"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_cluster(input_path, centroids):\n",
    "    encoder = Encoder(encoding=\"mobilenet\")\n",
    "    features = encoder(input_path)\n",
    "    dis = (-1.0, 0)\n",
    "    for i in range(centroids.shape[0]) :\n",
    "        a = cosine_similarity(features, centroids[i].reshape(1, -1))\n",
    "        if dis[0] < a :\n",
    "            dis = (a, i)\n",
    "    return dis[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2020-12-06 02:28:57,124: INFO Initialized: MobileNet pretrained on ImageNet dataset sliced at last conv layer and added GlobalAveragePooling\n",
      "The closest cluster is 0.\n",
      "Wall time: 6.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "input_path = IMAGESDIR + '00076.jpg'\n",
    "clust = get_closest_cluster(input_path, centroids)\n",
    "print(f\"The closest cluster is {clust}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_clustering():\n",
    "    centroids = pickle.load(open(\"./clusters/centroids.pkl\", \"rb\"))\n",
    "    labels = pickle.load(open(\"./clusters/labels.pkl\", \"rb\"))\n",
    "    data = pd.read_csv(\"./data/out.csv\")\n",
    "    data = pd.DataFrame({\n",
    "        \"duplicates\": data.duplicates.apply(lambda x: [int(elt) for elt in x.split(\" \")])\n",
    "    })\n",
    "    total_acc = 0.0\n",
    "    for clust in range(5):\n",
    "        ids = np.random.choice(np.where(labels == clust)[0], 5)\n",
    "        acc = 0.0\n",
    "        for id_ in ids:\n",
    "            targets = data.duplicates.iloc[id_]\n",
    "            tmp_acc = 0\n",
    "            for target in targets:\n",
    "                if target in np.where(labels == clust)[0]:\n",
    "                    tmp_acc += 1\n",
    "            tmp_acc = tmp_acc/len(targets)\n",
    "            acc += tmp_acc\n",
    "            print(f\"Cluster {clust} - image n°{id_} - Acc: {tmp_acc}\")\n",
    "        acc = acc/len(ids)\n",
    "        print(f\"Cluster {clust} - Acc: {acc}\")\n",
    "        total_acc += acc\n",
    "    \n",
    "    return total_acc/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cluster 0 - image n°239 - Acc: 1.0\nCluster 0 - image n°63 - Acc: 1.0\nCluster 0 - image n°171 - Acc: 1.0\nCluster 0 - image n°21 - Acc: 1.0\nCluster 0 - image n°94 - Acc: 1.0\nCluster 0 - Acc: 1.0\nCluster 1 - image n°12 - Acc: 1.0\nCluster 1 - image n°12 - Acc: 1.0\nCluster 1 - image n°54 - Acc: 1.0\nCluster 1 - image n°116 - Acc: 1.0\nCluster 1 - image n°152 - Acc: 1.0\nCluster 1 - Acc: 1.0\nCluster 2 - image n°162 - Acc: 1.0\nCluster 2 - image n°113 - Acc: 1.0\nCluster 2 - image n°93 - Acc: 1.0\nCluster 2 - image n°149 - Acc: 1.0\nCluster 2 - image n°52 - Acc: 1.0\nCluster 2 - Acc: 1.0\nCluster 3 - image n°217 - Acc: 1.0\nCluster 3 - image n°77 - Acc: 1.0\nCluster 3 - image n°183 - Acc: 1.0\nCluster 3 - image n°206 - Acc: 1.0\nCluster 3 - image n°137 - Acc: 1.0\nCluster 3 - Acc: 1.0\nCluster 4 - image n°4 - Acc: 1.0\nCluster 4 - image n°197 - Acc: 1.0\nCluster 4 - image n°4 - Acc: 1.0\nCluster 4 - image n°67 - Acc: 1.0\nCluster 4 - image n°131 - Acc: 1.0\nCluster 4 - Acc: 1.0\nClustering Acc: 1.0\nWall time: 13 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "acc = eval_clustering()\n",
    "print(f\"Clustering Acc: {acc}\")"
   ]
  },
  {
   "source": [
    "## Find duplicates or near images\n",
    "\n",
    "#### Get images' features in cluster"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = np.where(labels == clust)[0]"
   ]
  },
  {
   "source": [
    "#### Search for duplicates using MobileNet"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_duplicates_mn(input_path, indexes):\n",
    "    encoder = Encoder(encoding=\"mobilenet\")\n",
    "    target_features = encoder(input_path)\n",
    "    imgs_features = pickle.load(open(\"./features/features_mn.pkl\", \"rb\"))\n",
    "    duplicates = []\n",
    "    closest = [-1, -1]\n",
    "    for i in indexes:\n",
    "        similarity = cosine_similarity(target_features, imgs_features[i].reshape(1, -1))\n",
    "        if similarity > 0.85:\n",
    "            duplicates.append(i)\n",
    "        elif closest[0] < similarity:\n",
    "            closest[0] = similarity\n",
    "            closest[1] = i\n",
    "    \n",
    "    return duplicates, closest[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2020-12-06 02:28:59,673: INFO Initialized: MobileNet pretrained on ImageNet dataset sliced at last conv layer and added GlobalAveragePooling\n",
      "MobileNet\n",
      "----------\n",
      "Found images at [72, 75, 80, 98, 106] indexes as duplicates and the next closest image at 170.\n",
      "Wall time: 2.41 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "duplicates, closest = search_duplicates_mn(input_path, indexes)\n",
    "print(\"MobileNet\")\n",
    "print(\"-\"*10)\n",
    "print(f\"Found images at {duplicates} indexes as duplicates and the next closest image at {closest}.\")"
   ]
  },
  {
   "source": [
    "#### Search for duplicates using hashing methods"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_duplicates_hash(input_path, indexes, dirname, mode=\"phash\"):\n",
    "    encoder = Encoder(encoding=mode)\n",
    "    encoded_target = encoder(input_path)\n",
    "    duplicates = []\n",
    "    closest = [np.inf, -1]\n",
    "    filenames = os.listdir(dirname)\n",
    "    for i in indexes:\n",
    "        distance = encoder.metric(encoded_target, encoder(os.path.join(dirname, filenames[i])))\n",
    "        if distance <= 12:\n",
    "            duplicates.append(i)\n",
    "        elif closest[0] > distance:\n",
    "            closest[0] = distance\n",
    "            closest[1] = i\n",
    "    \n",
    "    return duplicates, closest[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PHash\n----------\nFound images at [72, 75, 80, 98, 106] indexes as duplicates and the next closest image at 95.\nWall time: 1.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "duplicates, closest = search_duplicates_hash(input_path, indexes, IMAGESDIR, mode=\"phash\")\n",
    "print(\"PHash\")\n",
    "print(\"-\"*10)\n",
    "print(f\"Found images at {duplicates} indexes as duplicates and the next closest image at {closest}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "AHash\n----------\nFound images at [72, 75, 80, 98, 106] indexes as duplicates and the next closest image at 3.\nWall time: 990 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "duplicates, closest = search_duplicates_hash(input_path, indexes, IMAGESDIR, mode=\"ahash\")\n",
    "print(\"AHash\")\n",
    "print(\"-\"*10)\n",
    "print(f\"Found images at {duplicates} indexes as duplicates and the next closest image at {closest}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "DHash\n----------\nFound images at [72, 75, 80, 98, 106] indexes as duplicates and the next closest image at 95.\nWall time: 1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "duplicates, closest = search_duplicates_hash(input_path, indexes, IMAGESDIR, mode=\"dhash\")\n",
    "print(\"DHash\")\n",
    "print(\"-\"*10)\n",
    "print(f\"Found images at {duplicates} indexes as duplicates and the next closest image at {closest}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WHash\n----------\nFound images at [3, 15, 64, 72, 75, 80, 95, 98, 106] indexes as duplicates and the next closest image at 58.\nWall time: 1.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "duplicates, closest = search_duplicates_hash(input_path, indexes, IMAGESDIR, mode=\"whash\")\n",
    "print(\"WHash\")\n",
    "print(\"-\"*10)\n",
    "print(f\"Found images at {duplicates} indexes as duplicates and the next closest image at {closest}.\")"
   ]
  },
  {
   "source": [
    "#### Search for duplicates using ORB descriptor"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_duplicates_orb(input_path, indexes, dirname):\n",
    "    encoder = Encoder(encoding=\"orb\")\n",
    "    encoded_target = encoder(input_path)\n",
    "    duplicates = []\n",
    "    closest = [0, -1]\n",
    "    filenames = os.listdir(dirname)\n",
    "    for i in indexes:\n",
    "        score = encoder.metric(encoded_target, encoder(os.path.join(dirname, filenames[i])))\n",
    "        if score >= 50:\n",
    "            duplicates.append(i)\n",
    "        elif closest[0] < score:\n",
    "            closest[0] = score\n",
    "            closest[1] = i\n",
    "    \n",
    "    return duplicates, closest[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ORB\n----------\nFound images at [36, 72, 75, 80, 98, 106] indexes as duplicates and the next closest image at 21.\nWall time: 1.97 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "duplicates, closest = search_duplicates_orb(input_path, indexes, IMAGESDIR)\n",
    "print(\"ORB\")\n",
    "print(\"-\"*10)\n",
    "print(f\"Found images at {duplicates} indexes as duplicates and the next closest image at {closest}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance(dirname, encoding):\n",
    "    total_precision = 0.0\n",
    "    total_recall = 0.0\n",
    "    for clust in range(5):\n",
    "        evaluator = Evaluator(\"./data/out.csv\")\n",
    "        id_ = np.random.choice(np.where(labels == clust)[0], 1)[0]\n",
    "        filename = os.listdir(dirname)[id_]\n",
    "        input_path = os.path.join(dirname, filename)\n",
    "        indexes = np.where(labels == clust)[0]\n",
    "        if encoding == \"mobilenet\":\n",
    "            duplicates, closest = search_duplicates_mn(input_path, indexes)\n",
    "        elif encoding in [\"phash\", \"ahash\", \"dhash\", \"whash\"]:\n",
    "            duplicates, closest = search_duplicates_hash(input_path, indexes, dirname, mode=encoding)\n",
    "        elif encoding == \"orb\":\n",
    "            duplicates, closest = search_duplicates_orb(input_path, indexes, dirname)\n",
    "        precision, recall, _ = evaluator.eval(id_, duplicates, indexes)\n",
    "        print(f\"Cluster {clust} - image n°{id_} - {duplicates} as duplicates - Precision: {precision} - Recall: {recall}\")\n",
    "        total_precision += precision\n",
    "        total_recall += recall\n",
    "    total_precision = total_precision/5\n",
    "    total_recall = total_recall/5\n",
    "    f1_score = 2*((total_precision*total_recall)/(total_precision+total_recall))\n",
    "    return total_precision, total_recall, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2020-12-06 02:29:08,698: INFO Initialized: MobileNet pretrained on ImageNet dataset sliced at last conv layer and added GlobalAveragePooling\n",
      "Cluster 0 - image n°64 - [15, 64] as duplicates - Precision: 1.0 - Recall: 1.0\n",
      "2020-12-06 02:29:10,855: INFO Initialized: MobileNet pretrained on ImageNet dataset sliced at last conv layer and added GlobalAveragePooling\n",
      "Cluster 1 - image n°230 - [230, 232] as duplicates - Precision: 1.0 - Recall: 1.0\n",
      "2020-12-06 02:29:15,808: INFO Initialized: MobileNet pretrained on ImageNet dataset sliced at last conv layer and added GlobalAveragePooling\n",
      "Cluster 2 - image n°162 - [162] as duplicates - Precision: 1.0 - Recall: 1.0\n",
      "2020-12-06 02:29:18,014: INFO Initialized: MobileNet pretrained on ImageNet dataset sliced at last conv layer and added GlobalAveragePooling\n",
      "Cluster 3 - image n°154 - [154, 195] as duplicates - Precision: 0.5 - Recall: 1.0\n",
      "2020-12-06 02:29:20,559: INFO Initialized: MobileNet pretrained on ImageNet dataset sliced at last conv layer and added GlobalAveragePooling\n",
      "Cluster 4 - image n°1 - [0, 1, 49, 57] as duplicates - Precision: 1.0 - Recall: 1.0\n",
      "MobileNet - Precision: 0.9 - Recall: 1.0 - F1 Score: 0.9473684210526316\n",
      "Wall time: 14.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "precision, recall, f1_score = performance(IMAGESDIR, encoding=\"mobilenet\")\n",
    "print(f\"MobileNet - Precision: {precision} - Recall: {recall} - F1 Score: {f1_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cluster 0 - image n°104 - [104] as duplicates - Precision: 1.0 - Recall: 1.0\n",
      "Cluster 1 - image n°223 - [222, 223, 226, 228] as duplicates - Precision: 1.0 - Recall: 1.0\n",
      "Cluster 2 - image n°117 - [117] as duplicates - Precision: 1.0 - Recall: 1.0\n",
      "Cluster 3 - image n°206 - [206, 207] as duplicates - Precision: 1.0 - Recall: 1.0\n",
      "Cluster 4 - image n°234 - [233, 234] as duplicates - Precision: 0.5 - Recall: 1.0\n",
      "PHash - Precision: 0.9 - Recall: 1.0 - F1 Score: 0.9473684210526316\n",
      "Wall time: 1min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "precision, recall, f1_score = performance(IMAGESDIR, encoding=\"phash\")\n",
    "print(f\"PHash - Precision: {precision} - Recall: {recall} - F1 Score: {f1_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cluster 0 - image n°95 - [3, 95] as duplicates - Precision: 0.5 - Recall: 1.0\n",
      "Cluster 1 - image n°12 - [12] as duplicates - Precision: 1.0 - Recall: 1.0\n",
      "Cluster 2 - image n°179 - [148, 179] as duplicates - Precision: 0.5 - Recall: 1.0\n",
      "Cluster 3 - image n°145 - [143, 145, 154, 195] as duplicates - Precision: 0.5 - Recall: 1.0\n",
      "Cluster 4 - image n°130 - [130, 131, 132] as duplicates - Precision: 1.0 - Recall: 1.0\n",
      "AHash - Precision: 0.7 - Recall: 1.0 - F1 Score: 0.8235294117647058\n",
      "Wall time: 1min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "precision, recall, f1_score = performance(IMAGESDIR, encoding=\"ahash\")\n",
    "print(f\"AHash - Precision: {precision} - Recall: {recall} - F1 Score: {f1_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cluster 0 - image n°239 - [239] as duplicates - Precision: 1.0 - Recall: 1.0\n",
      "Cluster 1 - image n°37 - [6, 37, 46, 54] as duplicates - Precision: 0.25 - Recall: 1.0\n",
      "Cluster 2 - image n°153 - [153] as duplicates - Precision: 1.0 - Recall: 1.0\n",
      "Cluster 3 - image n°23 - [23, 40] as duplicates - Precision: 1.0 - Recall: 1.0\n",
      "Cluster 4 - image n°234 - [233, 234] as duplicates - Precision: 0.5 - Recall: 1.0\n",
      "DHash - Precision: 0.75 - Recall: 1.0 - F1 Score: 0.8571428571428571\n",
      "Wall time: 1min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "precision, recall, f1_score = performance(IMAGESDIR, encoding=\"dhash\")\n",
    "print(f\"DHash - Precision: {precision} - Recall: {recall} - F1 Score: {f1_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cluster 0 - image n°182 - [170, 171, 182] as duplicates - Precision: 0.3333333333333333 - Recall: 1.0\n",
      "Cluster 1 - image n°226 - [222, 223, 226, 228, 230, 232] as duplicates - Precision: 0.6666666666666666 - Recall: 1.0\n",
      "Cluster 2 - image n°196 - [166, 196] as duplicates - Precision: 0.5 - Recall: 1.0\n",
      "Cluster 3 - image n°56 - [56] as duplicates - Precision: 1.0 - Recall: 1.0\n",
      "Cluster 4 - image n°220 - [220] as duplicates - Precision: 1.0 - Recall: 0.25\n",
      "WHash - Precision: 0.7 - Recall: 0.85 - F1 Score: 0.767741935483871\n",
      "Wall time: 1min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "precision, recall, f1_score = performance(IMAGESDIR, encoding=\"whash\")\n",
    "print(f\"WHash - Precision: {precision} - Recall: {recall} - F1 Score: {f1_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cluster 0 - image n°119 - [36, 119] as duplicates - Precision: 0.5 - Recall: 1.0\n",
      "Cluster 1 - image n°12 - [12] as duplicates - Precision: 1.0 - Recall: 1.0\n",
      "Cluster 2 - image n°203 - [158, 203] as duplicates - Precision: 0.5 - Recall: 1.0\n",
      "Cluster 3 - image n°48 - [22, 48] as duplicates - Precision: 1.0 - Recall: 1.0\n",
      "Cluster 4 - image n°161 - [161, 202] as duplicates - Precision: 0.5 - Recall: 1.0\n",
      "ORB - Precision: 0.7 - Recall: 1.0 - F1 Score: 0.8235294117647058\n",
      "Wall time: 1min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "precision, recall, f1_score = performance(IMAGESDIR, encoding=\"orb\")\n",
    "print(f\"ORB - Precision: {precision} - Recall: {recall} - F1 Score: {f1_score}\")"
   ]
  }
 ]
}